<!DOCTYPE html
  PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>AlignFormer</title>
  <!--=================Meta tags==========================-->
  <meta name="robots" content="index,follow">
  <meta name="keywords" content="image restoration, UDC">
  <link rel="author" href="https://jnjaby.github.io/projects/UDC">
  <!--=================js==========================-->
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <link href="./css.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" type="text/css" href="../project.css" media="screen">
  <script src="./effect.js "></script>
  <!-- Latex -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]},
        TeX: { equationNumbers: { autoNumber: "AMS" } },
      });
      </script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
    </script>
  <!--=================Google Analytics==========================-->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-129775907-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());

    gtag('config', 'UA-129775907-1');
  </script>
</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section head">
        <h1>
          Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera
        </h1>
        <!-- <h1>
          
        </h1> -->
        <!--=================Authors==========================-->
        <div class="authors">
          <a href="https://jnjaby.github.io/" target="_blank">Ruicheng Feng</a> <sup>1</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://li-chongyi.github.io/" target="_blank">Chongyi Li</a> <sup>1</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://hc25.web.rice.edu/" target="_blank">Huaijin Chen</a> <sup>2</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://scholar.google.com/citations?user=mFG5Sv4AAAAJ&hl=en/" target="_blank">Shuai Li</a>
          <sup>2</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://www.gujinwei.org/" target="_blank">Jinwei Gu</a> <sup>3,4</sup>
          &nbsp;&nbsp;&nbsp;&nbsp;
          <a href="https://personal.ie.cuhk.edu.hk/~ccloy/" target="_blank">
            Chen Change Loy</a> <sup>1</sup>
        </div>

        <div class="affiliations ">
          <sup>1</sup> S-Lab, Nanyang Technological University<br>
          <sup>2</sup> SenseBrain Technology<br>
          <sup>3</sup> The Chinese University of Hong Kong<br>
          <sup>4</sup> Shanghai AI Laboratory
        </div>
        <!--=================Tabs==========================-->
        <ul id="tabs">
          <li><a href="#abstract" name="#tab1">Abstract</a></li>
          <li><a href="#dataset" name="#tab2">Dataset</a></li>
          <li><a href="#method" name="#tab3">Method</a></li>
          <li><a href="#results" name="#tab4">Results</a></li>
          <li><a href="#citation" name="#tab5">Citation</a></li>
      </div>
      <br>
      <!--=================Teasers==========================-->
      <div class="container">
        <div>
          <table width="90%" align="center" cellspacing="0" cellpadding="30px">
            <tbody>
              <tr>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_1.png" onmouseover="this.src='src/AF_1.png';" onmouseout="this.src='src/UDC_1.png';"
                    height="280" width="280">
                </td>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_2.png" onmouseover="this.src='src/AF_2.png';" onmouseout="this.src='src/UDC_2.png';"
                    height="280" width="280">
                </td>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_3.png" onmouseover="this.src='src/AF_3.png';" onmouseout="this.src='src/UDC_3.png';"
                    height="280" width="280">
                </td>
              </tr>
              <tr>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_4.png" onmouseover="this.src='src/AF_4.png';" onmouseout="this.src='src/UDC_4.png';"
                    height="280" width="280">
                </td>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_5.png" onmouseover="this.src='src/AF_5.png';" onmouseout="this.src='src/UDC_5.png';"
                    height="280" width="280">
                </td>
                <td style="padding:0 10px 10px 10px;">
                  <img src="src/UDC_6.png" onmouseover="this.src='src/AF_6.png';" onmouseout="this.src='src/UDC_6.png';"
                    height="280" width="280">
                </td>
              </tr>
            </tbody>
          </table>
        </div>
        <p align="center">
          <b>MouseOut</b>: UDC images&emsp;&emsp;&emsp;&emsp;
          <b>MouseOver</b>: AlignFormer results
        </p>
      </div>

      <!--=================Abstract==========================-->
      <div class="section abstract" , id="abstract">
        <h2>Abstract</h2>
        <p>
          <br>
          Due to the difficulty in collecting large-scale and perfectly aligned paired training data for Under-Display
          Camera (UDC) image restoration, previous methods resort to
          monitor-based image systems or simulation-based methods,
          sacrificing the realness of the data and introducing domain
          gaps. In this work, we revisit the classic stereo setup for
          training data collection – capturing two images of the same
          scene with one UDC and one standard camera. The key
          idea is to “copy” details from a high-quality reference image and “paste” them on the UDC image. While being
          able to generate real training pairs, this setting is susceptible to spatial misalignment due to perspective
          and depth
          of field changes. The problem is further compounded by
          the large domain discrepancy between the UDC and normal
          images, which is unique to UDC restoration. In this paper,
          we mitigate the non-trivial domain discrepancy and spatial
          misalignment through a novel Transformer-based framework that generates well-aligned yet high-quality target
          data for the corresponding UDC input. This is made possible through two carefully designed components, namely,
          the
          Domain Alignment Module (DAM) and Geometric Alignment Module (GAM), which encourage robust and accurate
          discovery of correspondence between the UDC and normal views. Extensive experiments show that high-quality
          and well-aligned pseudo UDC training pairs are beneficial for training a robust restoration network.
        </p>
      </div>



      <!--=================Materials==========================-->
      <div class="section materials" , id="materials">
        <h2>Materials</h2>
        <br>
        <table width="100%" align="center" border=none cellspacing="0" cellpadding="30">
          <tr>
            <td width="30%">
              <center>
                <a href="https://arxiv.org/abs/2304.06019" target="_blank" class="imageLink"><img
                    src="./src/thumbnail.png" , height="120"></a><br><br>
                <a href="https://arxiv.org/abs/2304.06019" disabled target="_blank">Paper</a>
              </center>
            </td>
            <td width="30%">
              <center>
                <a href="https://github.com/jnjaby/AlignFormer" target="_blank" class="imageLink"><img
                    src="../icon_dataset.png" , height="120"></a><br><br>
                <a href="https://github.com/jnjaby/AlignFormer" target="_blank">Data</a>
              </center>
            </td>
            <td width="30%" valign="middle">
              <center>
                <a href="https://github.com/jnjaby/AlignFormer" target="_blank" class="imageLink"><img
                    src="../icon_github.png" , height="120"></a><br><br>
                <a href="https://github.com/jnjaby/AlignFormer" target="_blank">Codes</a>
              </center>
            </td>
          </tr>
        </table>
      </div>
    </div>


    <!--=================Dataset==========================-->
    <div class="section" , id="dataset">
      <h2>Dataset</h2>
      <p>
        We provide the dataset for Under-Display Camera Images. Train and validation subsets are
        publicly available. Downloads are available via Google Drive or running the python code.
      </p>
      <p>
        To build the degraded-reference image pairs, we construct a stereo
        smartphone array - ZTE Axon 20 with selfie under-display
        camera, and iPhone 13 Pro rear camera, which are physically aligned as much as possible. To eliminate the
        effects of built-in ISPs,
        both UDC and Ref images are extracted from raw dump of data with minimal processing (demosaic and gamma
        correction) and converted into sRGB domain. In total, we
        collect 330 image pairs covering both indoor and outdoor scenes.
      </p>
    </div>


    <!--=================Method==========================-->
    <div class="section" , id="method">
      <h2>Method</h2>
      <div style="text-align: center; vertical-align:middle">
        <img src="src/architecture.png" width="350">
      </div>
      <div align="center">
        Overview of the proposed AlignFormer.
      </div>
      <p>
        We first mitigate domain discrepancy between UDC image and reference
        image via Domain Alignmain Module (DAM) to obtain domain-corrected image,
        which are then gathered with reference image and fed into two U-shape CNNs
        for feature extraction. Then the features at each scale are attended
        by the Geometric Alignment Transformer (GAM) to obtain the
        output features, which will be processed and fused in another UNet to produce the pseudo image.
      </p>
      <br>
      <div style="text-align: center; vertical-align:middle">
        <img src="src/DAM.png" width="350">
      </div>
      <div align="center">
        The structure of Domain Alignment Module (DAM).
      </div>
      <p>
        This module comprises a guidance net and a matching net. The guidance vector generated by the guidance net is
        used for style modulation via StyleConv in the matching net. Such designs help to
        imitate the color and illuminance of reference image, while preserving spatial information of UDC image.
      </p>
      <br>
      <div style="text-align: center; vertical-align:middle">
        <img src="src/GAM.png" width="350">
      </div>
      <div align="center">
        The structure of Geometric Alignment Module (GAM).
      </div>
      <p>
        GAM is to exploit the pixel correspondences by incorporating geometric cues in attention.
        Specifically, we use an off-the-shelf flow estimator as the
        guidance to sample features from the vicinity in the reference image. We introduce a flow estimator prior in the
        conventional attention as it can exploit the geometric prior to
        facilitate the subsequent attention mechanism.
      </p>
    </div>


    <!--=================Results==========================-->
    <div class="section" , id="results">
      <h2>Results</h2>
      <br>
      <table>
        <tr>
          <div style="text-align: center; vertical-align:middle">
            <img src="src/vis_1" width="780">
          </div>
        </tr>
        <br>
        <tr>
          <div style="text-align: center; vertical-align:middle">
            <img src="src/vis_2" width="780">
          </div>
        </tr>
        <tr>
          <div align="center">
            Visual results between different dataset on PPM-UNet.
          </div>
        </tr>
      </table>
    </div>

    <!--=================License==========================-->
    <div class="section" , id="License">
      <h2>License</h2>
      <p>
        This project is open sourced under
        <a href="https://github.com/jnjaby/AlignFormer/blob/main/LICENSE" target="_blank">NTU S-Lab License 1.0</a>.
        Redistribution and use should follow this license.
      </p>
    </div>


    <!--=================Citation==========================-->
    <div class="section citation" , id="citation">
      <h2>Citation</h2>
      If you find our dataset and paper useful for your research, please consider citing our work:
      <div class="section bibtex">
        <pre>
@InProceedings{Feng_2023_Generating,
    author = {Feng, Ruicheng and Li, Chongyi and Chen, Huaijin and Li, Shuai and Gu, Jinwei and Loy, Chen Change},
    title = {Generating Aligned Pseudo-Supervision from Non-Aligned Data for Image Restoration in Under-Display Camera},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month = {June},
    year = {2023},
}
          </pre>
      </div>
    </div>

    <!--=================Contact==========================-->
    <div class="section contact">
      <h2 id="contact">Contact</h2>
      <p>If you have any question, please contact us via <strong>ruicheng002@ntu.edu.sg</strong>.
      </p>
      <br>
    </div>
</body>

</html>